import os
import glob
import streamlit as st
import PyPDF2
import docx
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from tenacity import retry, stop_after_attempt, wait_exponential
import google.generativeai as genai

# -------------------------------
# Load text from pre-provided books
# -------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def extract_text(path: str) -> str:
    text = ""
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".pdf":
            with open(path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += (page.extract_text() or "") + "\n"
        elif ext in (".docx", ".doc"):
            doc = docx.Document(path)
            for para in doc.paragraphs:
                text += para.text + "\n"
    except Exception as e:
        # show file-specific error but continue
        st.error(f"Error reading {os.path.basename(path)}: {e}")
    return text


@st.cache_data(show_spinner=False, ttl=3600)
def load_documents():
    base_dir = os.path.dirname(__file__) if "__file__" in globals() else os.getcwd()
    books_dir = os.path.join(base_dir, "books")

    paths = glob.glob(os.path.join(books_dir, "*.pdf")) + glob.glob(os.path.join(books_dir, "*.docx"))

    if not paths:
        st.warning("ğŸ“ 'books/' ÙÙˆÙ„ÚŠØ± Ø®Ø§Ù„ÙŠ Ø¢Ú¾ÙŠ. Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ PDF ÙŠØ§ DOCX ÙØ§Ø¦Ù„ÙˆÙ† Ø´Ø§Ù…Ù„ ÚªØ±ÙŠÙˆ.")
        st.stop()

    chunks = []
    for path in paths:
        text = extract_text(path)
        if text.strip():
            # split long text into 1000-character chunks for TF-IDF indexing
            split_chunks = [text[i : i + 1000] for i in range(0, len(text), 1000)]
            chunks.extend(split_chunks)

    if not chunks:
        st.warning("ğŸ“‚ ÙØ§Ø¦Ù„Ù† Ù…Ø§Ù† ÚªÙˆØ¨Ù‡ Ù¾Ú™Ù‡Ú» Ù„Ø§Ø¦Ù‚ Ù…ÙˆØ§Ø¯ Ù†Ø§Ú¾ÙŠ.")
        st.stop()
    return chunks


# -------------------------------
# TF-IDF Retriever
# -------------------------------
@st.cache_resource(show_spinner=False)
def build_retriever():
    texts = load_documents()
    vectorizer = TfidfVectorizer().fit(texts)
    vectors = vectorizer.transform(texts)

    def retrieve(query: str, k: int = 3):
        query_vec = vectorizer.transform([query])
        sims = cosine_similarity(query_vec, vectors)[0]
        top_idx = np.argsort(sims)[-k:][::-1]
        return [texts[i] for i in top_idx if sims[i] > 0.01]

    return retrieve


# -------------------------------
# Google Gemini LLM wrapper
# -------------------------------
class GoogleGeminiLLM:
    def __init__(self):
        cfg = st.secrets.get("openai_gemma", {})
        self.api_key = cfg.get("api_key")
        self.model = cfg.get("model", "gemini-1.5-flash")

        if not self.api_key:
            st.error("âš ï¸ Gemini API key missing in secrets.toml under [openai_gemma].")
            st.stop()

        genai.configure(api_key=self.api_key)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def call(self, prompt: str) -> str:
        try:
            model = genai.GenerativeModel(self.model)
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            st.error(f"Google API error: {e}")
            raise


# -------------------------------
# QA Function (Retriever + LLM)
# -------------------------------
@st.cache_resource(show_spinner=False)
def get_qa_chain():
    retrieve = build_retriever()
    llm = GoogleGeminiLLM()

    system_prompt = (
        "Ø§ÙˆÚ¾Ø§Ù† ØµØ­Øª Ø¨Ø§Ø¨Øª Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ ÚÙŠÙ†Ø¯Ú™ Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¢Ú¾ÙŠÙˆ.\n"
        "ÙˆØ§Ù¾ÙŠØ¯Ø§Ø± Ø§ÙˆÚ¾Ø§Ù† Ú©Ø§Ù† ØµØ­Øª Ø¨Ø§Ø¨Øª Ø³ÙˆØ§Ù„ Ù¾Ú‡Ù†Ø¯Ø§ Ø¢Ú¾Ù† Û½ Ø§ÙˆÚ¾Ø§Ù† Ú©ÙŠ ØµØ±Ù books ÙÙˆÙ„ÚŠØ± Ù…Ø§Ù† Ú„Ø§Ú» Ø§Ø³ØªØ¹Ù…Ø§Ù„ ÚªØ±ÙŠ Ø¬ÙˆØ§Ø¨ ÚÙŠÚ»Ø§ Ø¢Ú¾Ù†.\n"
        "Ø¬ÙˆØ§Ø¨ ØµØ±Ù Ø³Ù†ÚŒÙŠ Û¾ ÚÙŠÙˆØŒ Ø§Ø­ØªØ±Ø§Ù…ØŒ Ø³Ø§Ø¯Ú¯ÙŠ Û½ ÙˆØ¶Ø§Ø­Øª Ø³Ø§Ù†.\n"
        "ØºÙŠØ± Ø§Ø®Ù„Ø§Ù‚ÙŠØŒ Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙŠØ§ ØºÙŠØ± Ù…ØªØ¹Ù„Ù‚ Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ Ù†Ù‡ ÚÙŠÙˆ.\n"
    )

    def qa(inputs):
        docs = retrieve(inputs["query"])
        context = "\n".join(docs)
        prompt = f"{system_prompt}\n\nContext:\n{context}\n\nØ³ÙˆØ§Ù„: {inputs['query']}\n\nØ¬ÙˆØ§Ø¨:"
        return {"result": llm.call(prompt)}

    return qa


# -------------------------------
# Streamlit Chat UI (main)
# -------------------------------
def main():
    st.set_page_config(page_title="ØµØ­Øª Ú†ÙŠÙ½ Ø¨ÙˆÙ½", layout="centered")
    st.title("ğŸ©º ØµØ­Øª Ø¨Ø§Ø¨Øª Ú†ÙŠÙ½ Ø¨ÙˆÙ½")
    st.markdown(" Sindhi health Q&A â€” Ø³ÙˆØ§Ù„ Ù¾Ú‡Ùˆ Û½ ÚªØªØ§Ø¨Ù† Ù…Ø§Ù† Ø¬ÙˆØ§Ø¨ Ø­Ø§ØµÙ„ ÚªØ±ÙŠÙˆ.")
    st.divider()

    # session messages (preserve chat history)
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # render previous messages
    for msg in st.session_state.messages:
        # msg["role"] should be either "user" or "assistant"
        display_role = "ğŸ™‚ ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø±" if msg["role"] == "user" else "ğŸ¤– Ú†ÙŠÙ½ Ø¨ÙˆÙ½"
        with st.chat_message(msg["role"]):
            st.markdown(f"**{display_role}:**\n{msg['content']}")

    # Suggested quick questions (two buttons)
    st.markdown("### ØªØ¬ÙˆÙŠØ² ÚªÙŠÙ„ Ø³ÙˆØ§Ù„ (ØªÚªÚ™Ùˆ Ú†ÙˆÙ†ÚŠÙŠÙˆ):")
    cols = st.columns([1, 1])
    q1 = "Ø±ÙˆØ²Ø§Ù†ÙŠ Ø¬Ø³Ù…Ø§Ù†ÙŠ Ù…Ø´Ù‚ Ø¬Ø§ ÙØ§Ø¦Ø¯Ø§ Ú‡Ø§ Ø¢Ú¾Ù†ØŸ"
    q2 = "ØµØ­Øª Ù…Ù†Ø¯ ØºØ°Ø§ Û¾ ÚªÚ¾Ú™Ø§ Ú©Ø§ÚŒØ§ Ø´Ø§Ù…Ù„ ÚªØ±Ú» Ú¯Ú¾Ø±Ø¬Ù†ØŸ"

    clicked_question = None
    if cols[0].button(q1):
        clicked_question = q1
    if cols[1].button(q2):
        clicked_question = q2

    # Get input: if user clicked a suggested question, use it as input; otherwise show chat_input
    if clicked_question:
        user_input = clicked_question
    else:
        user_input = st.chat_input("Ù¾Ù†Ú¾Ù†Ø¬Ùˆ Ø³ÙˆØ§Ù„ Ù„Ú©Ùˆ...")

    # Process input
    if user_input and user_input.strip():
        # append user message
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(f"**ğŸ™‚ ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø±:**\n{user_input}")

        # compute and show assistant reply
        with st.spinner("Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¬ÙˆØ§Ø¨ ØªÙŠØ§Ø± ÚªØ±ÙŠ Ø±Ù‡ÙŠÙˆ Ø¢Ù‡ÙŠ..."):
            try:
                qa = get_qa_chain()
                result = qa({"query": user_input})
                answer = result.get("result", "Ù…Ø¹Ø§Ù ÚªØ¬ÙˆØŒ Ù…Ø§Ù† Ú¾Ù† Ø³ÙˆØ§Ù„ Ø¬Ùˆ Ø¬ÙˆØ§Ø¨ Ù†Ù¿Ùˆ ÚØ¦ÙŠ Ø³Ú¯Ù‡Ø§Ù†.")
            except Exception as e:
                # show safe fallback if LLM or retrieval fails
                st.error(f"âŒ Ø®Ø§Ù…ÙŠ Ù¾ÙŠØ´ Ø¢Ø¦ÙŠ: {e}")
                answer = "Ù…Ø¹Ø§Ù ÚªØ¬ÙˆØŒ Ú¾Úª Ù½ÙŠÚªÙ†ÙŠÚªÙŠ Ù…Ø³Ø¦Ù„Ùˆ Ù¾ÙŠØ´ Ø¢ÙŠÙˆ. Ù…Ú¾Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ Ø¨Ø¹Ø¯ Û¾ ÚªÙˆØ´Ø´ ÚªØ±ÙŠÙˆ."

            # append and display assistant message
            st.session_state.messages.append({"role": "assistant", "content": answer})
            with st.chat_message("assistant"):
                st.markdown(f"**ğŸ¤– Ú†ÙŠÙ½ Ø¨ÙˆÙ½:**\n{answer}")

    # Sidebar information & disclaimer
    st.sidebar.title("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª")
    st.sidebar.markdown(
        """
- ğŸ“š Ø¬ÙˆØ§Ø¨ 'books/' ÙÙˆÙ„ÚŠØ± Ù…Ø§Ù† Ø­Ø§ØµÙ„ ÚªÙŠÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªÙŠ Ù…Ø¨Ù†ÙŠ Ø¢Ú¾Ù†.
- âš ï¸ **Disclaimer:** Ù‡ÙŠ ØµØ±Ù ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„Ø§Ø¡Ù Ø¢Ú¾ÙŠ â€” Ø·Ø¨ÙŠ Ù…Ø³Ø¦Ù„Ù† Ù„Ø§Ø¡Ù Ù…Ú¾Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ ØªØµØ¯ÙŠÙ‚ Ù¿ÙŠÙ„ ØµØ­Øª Ù…Ø§Ù‡Ø± Ø³Ø§Ù† Ø±Ø¬ÙˆØ¹ ÚªØ±ÙŠÙˆ.
"""
    )
    st.sidebar.divider()
    st.sidebar.caption("Developed by: Muhammad Faisal Jamali Â© 2025")


if __name__ == "__main__":
    main()
