import os
import glob
import streamlit as st
import PyPDF2
import docx
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.embeddings.base import Embeddings
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import Optional
from tenacity import retry, stop_after_attempt, wait_exponential
import google.generativeai as genai
from google.api_core.exceptions import GoogleAPIError

# -------------------------------
# Load Text from Files
# -------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def extract_text(path: str) -> str:
    text = ""
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".pdf":
            with open(path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += (page.extract_text() or "") + "\n"
        elif ext in (".docx", ".doc"):
            doc = docx.Document(path)
            for para in doc.paragraphs:
                text += para.text + "\n"
    except Exception as e:
        st.error(f"Error reading {path}: {e}")
    return text

@st.cache_data(show_spinner=False, ttl=3600)
def load_documents() -> list[Document]:
    paths = glob.glob("books/*.pdf") + glob.glob("books/*.docx")
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    docs = []
    for path in paths:
        raw = extract_text(path)
        if raw:
            chunks = splitter.split_text(raw)
            docs.extend(
                Document(page_content=chunk, metadata={"source": os.path.basename(path), "chunk": i})
                for i, chunk in enumerate(chunks)
            )
    return docs

# -------------------------------
# TF-IDF Custom Embedding
# -------------------------------
class CustomEmbeddings(Embeddings):
    def __init__(self, corpus: list[str]):
        self.vectorizer = TfidfVectorizer()
        self.vectorizer.fit(corpus)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self.vectorizer.transform(texts).toarray().tolist()

    def embed_query(self, text: str) -> list[float]:
        return self.vectorizer.transform([text]).toarray()[0].tolist()

@st.cache_resource(show_spinner=False)
def get_vectorstore():
    persist_dir = "./chroma_db"
    collection_name = "books"
    docs = load_documents()
    corpus = [d.page_content for d in docs]
    embeddings = CustomEmbeddings(corpus)

    if os.path.isdir(persist_dir) and os.listdir(persist_dir):
        return Chroma(persist_directory=persist_dir,
                      embedding_function=embeddings,
                      collection_name=collection_name)
    else:
        return Chroma.from_documents(docs,
                                     embeddings,
                                     persist_directory=persist_dir,
                                     collection_name=collection_name)

# -------------------------------
# Google Gemini LLM
# -------------------------------
class GoogleGeminiLLM:
    def __init__(self):
        cfg = st.secrets.get("openai_gemma", {})
        self.api_key = cfg.get("api_key")
        self.model = cfg.get("model", "gemini-1.5-flash")

        if not self.api_key:
            st.error("Missing API key for Gemini in secrets.toml.")
            st.stop()

        genai.configure(api_key=self.api_key)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def call(self, prompt: str) -> str:
        try:
            model = genai.GenerativeModel(self.model)
            response = model.generate_content(prompt)
            return response.text
        except GoogleAPIError as e:
            st.error(f"Google API error: {e}")
            raise

@st.cache_resource(show_spinner=False)
def get_qa_chain():
    vectorstore = get_vectorstore()
    llm = GoogleGeminiLLM()

    system_prompt = """
ุงูฺพุงู ุตุญุช ุจุงุจุช ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูู
ูุงููพูุฏุงุฑ ุงููุงู ฺฉุงู ุตุญุช ุจุงุจุช ุณูุงู ูพฺูุฏุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู
ุณููุฑุง ุฌูุงุจ books ูุงูู ูููฺุฑ ูุงู ฺูู

ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูู
ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุ ุบูุฑ ุถุฑูุฑู ฝ ุบูุฑ ูุงูููู ุณูุงู ูพฺู ุณฺฏฺพู ูฟุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ูุงฺพู ฺูฺปุง
ุงูฺพุงู ฺฉู ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู ุฌฺฺพู ุช ูุงููพูุฏุงุฑ ฺฉู ููุถูุน ุชู ุฑฺพฺป ุฌู ุชูููู ฝ ุญูุตูุง ุงูุฒุงุฆู ฺชุฑูู
ููุถูุน ฺฉุงู ูปุงฺพุฑ ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺป ุณุฎุชู ุณุงู ููุน ุขฺพู 
ูุงููพูุฏุงุฑ ุฌุฏูุฏ ูฝูฺชูุงูุงุฌู ฺฉุง ูุงูู ูุงฺพู
ุงูฺพุงู ฺฉู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู 
ูุงููพูุฏุงุฑ ุงฺป ูพฺฺพูู ฝ ูฝูฺชููฺชู ุงุตุทูุงุญู ฺฉุงู ุบูุฑ ูุงูู ุขฺพู 
ุงูฺพุงู ฺฉู ุขุณุงู ฝ ุนุงู ููู ุฒุจุงู พุฌูุงุจ ฺูฺป ฺฏฺพุฑุฌู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุฑููู ุงุฎุชูุงุฑ ฺชุฑู ูฟู ุช ุงูฺพุงู ฺฉู ุงุฎูุงู ุณุงู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู

ุงูฺพุงู ฺฉู ุณฺูู ุณูุงูู ุฌุง ุฌูุงุจ ุณูฺู ุฒุจุงู ฝ ุฑุณู ุงูุฎุท พ ฺูฺปุง ุขฺพู
ุณูฺู ฺฏุฑุงูุฑ ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู
ุฌูุงุจ พ ููุทู ฝ ููุธู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุฌูุงุจ ุตุญูุญ ุทุฑููู ฝ ุชุฑุชูุจ ุณุงู ฺพุฆฺป ฺฏฺพุฑุฌู 
ุฌูุงุจ พ ฺพุฑ ุทุฑุญ ุฌู ููุธูุ ุงููุงุก ฝ ุตูุฑุชุฎุทูุกู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุงฺฏุฑ ุณูุงู ุณูฺู ุฒุจุงู ฺฉุงู ุณูุงุกู ฺชูฺพู ูปู ุฒุจุงู พ ุงฺู ุช ุชฺฺพู ุจ ุฌูุงุจ ุณูฺู ุฒุจุงู พ ฺูู
ุงูฺพุงู ฺฉู ฺพุฑ ุฌูุงุจ พ ุงุญุชุฑุงู ุฌู ูุธุงฺพุฑู ฺชุฑฺปู ุขฺพู 
ูุงููพูุฏุงุฑ ุณุงู ุนุฒุช ฝ ุงุญุชุฑุงู ุณุงู ูพูุด ุงฺู
ุงุฎูุงููุงุช ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู 
ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑูู
ูุฑููุกู ุณุงู ุฌูุงุจ ฺูู
ุตุงุฑููู ฺฉู ฺชุชุงุจ ูููฺุฑ ุจุงุจุช ูู ูปฺุงูู.
ฺชูฺพู ุจ ุบูุท ุณูุงู ุฌู ุฌูุงุจ ุนุฒุช ุณุงู ฺูู
ูุงููพูุฏุงุฑู ฺฉู ูพูฺพูุฌู ุจูุงูุชุ ูฝูฺชููฺชู ุงุตุทูุงุญู ฝ ูุงฺู ุจุงุจุช ฺุงฺป ูู ฺูู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุงูฺพุงูุฌู ุจูุงูุช ุจุงุจุช ุณูุงู ฺชุฑู ุช ุงู ฺฉู ุตุฑู ุงฺพู ูปฺุงูู ุช ูุงู ูุตููุนู ุฐฺพุงูุช ุฌู ุงุตููู ุชู ูบฺพูู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูุงู.
"""

    prompt_template = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt),
        HumanMessagePromptTemplate.from_template("{context}\n\nุณูุงู: {question}")
    ])

    def qa_function(inputs):
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.get_relevant_documents(inputs["query"])
        context = "\n".join(d.page_content for d in docs)
        final_prompt = prompt_template.format(context=context, question=inputs["query"])
        return {"result": llm.call(final_prompt)}

    return qa_function

# -------------------------------
# Chat Interface
# -------------------------------
def main():
    st.set_page_config(page_title="ุตุญุช ฺููฝ ุจููฝ", layout="centered")
    st.title("๐ฉบ ุตุญุช ุจุงุจุช ฺููฝ ุจููฝ")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        role = "๐ค ฺููฝ ุจููฝ" if msg["role"] == "assistant" else "๐ ูุงููพูุฏุงุฑ"
        st.chat_message(msg["role"]).markdown(f"**{role}:**\n{msg['content']}")

    user_input = st.chat_input("ูพูฺพูุฌู ุณูุงู ูฺฉู...")
    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(f"**๐ ูุงููพูุฏุงุฑ:**\n{user_input}")

        with st.spinner("ฺููฝ ุจููฝ ุฌูุงุจ ฺุฆู ุฑููู ุขูู..."):
            try:
                qa = get_qa_chain()
                result = qa({"query": user_input})
                answer = result.get("result", "ูุนุงู ฺชุฌูุ ูุงู ฺพู ุณูุงู ุฌู ุฌูุงุจ ููฟู ฺุฆู ุณฺฏูุงู.")
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.chat_message("assistant").markdown(f"**๐ค ฺููฝ ุจููฝ:**\n{answer}")
            except Exception as e:
                st.error(f"โ ุฎุงูู ูพูุด ุขุฆู: {e}")

if __name__ == "__main__":
    main()