import os
import glob
import streamlit as st
import PyPDF2
import docx
from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.embeddings.base import Embeddings
from sklearn.feature_extraction.text import TfidfVectorizer
from tenacity import retry, stop_after_attempt, wait_exponential
import google.generativeai as genai
from google.api_core.exceptions import GoogleAPIError

# -------------------------------
# Load Text from Files
# -------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def extract_text(path: str) -> str:
    text = ""
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".pdf":
            with open(path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += (page.extract_text() or "") + "\n"
        elif ext in (".docx", ".doc"):
            doc = docx.Document(path)
            for para in doc.paragraphs:
                text += para.text + "\n"
    except Exception as e:
        st.error(f"Error reading {path}: {e}")
    return text

@st.cache_data(show_spinner=False, ttl=3600)
def load_documents() -> list[Document]:
    books_dir = os.path.join(os.path.dirname(__file__), "books")
    pdfs = glob.glob(os.path.join(books_dir, "*.pdf"))
    docxs = glob.glob(os.path.join(books_dir, "*.docx"))
    paths = pdfs + docxs

    st.write("ğŸ“„ Found files:", paths)  # For debug
    st.write("ğŸ“‚ Current working directory:", os.getcwd())
    st.write("ğŸ“‚ 'books' folder exists?", os.path.isdir(books_dir))

    if not paths:
        st.warning("ğŸ“ 'books/' ÙÙˆÙ„ÚŠØ± Ø®Ø§Ù„ÙŠ Ø¢Ú¾ÙŠ. Ù…Ù‡Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ ÚªØ¬Ù‡Ù‡ PDF ÙŠØ§ DOCX ÙØ§Ø¦Ù„ÙˆÙ† Ø´Ø§Ù…Ù„ ÚªØ±ÙŠÙˆ.")
        st.stop()

    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    docs = []
    for path in paths:
        raw = extract_text(path)
        if raw.strip():
            chunks = splitter.split_text(raw)
            docs.extend(
                Document(page_content=chunk, metadata={"source": os.path.basename(path), "chunk": i})
                for i, chunk in enumerate(chunks)
            )

    if not docs:
        st.warning("ğŸ“‚ Ø¯Ø³ØªØ§ÙˆÙŠØ²Ù† Ù…Ø§Ù† ÚªÙˆØ¨Ù‡ Ù‚Ø§Ø¨Ù„Ù Ù¾Ú™Ù‡Ú» Ù…ÙˆØ§Ø¯ Ù†Ø§Ú¾ÙŠ.")
        st.stop()
    return docs

# -------------------------------
# TF-IDF Custom Embedding (Robust)
# -------------------------------
class CustomEmbeddings(Embeddings):
    def __init__(self, corpus):
        clean_corpus = [c.strip() for c in corpus if c.strip()]
        if not clean_corpus:
            raise ValueError("Ø¯Ø³ØªØ§ÙˆÙŠØ²Ù† Ø®Ø§Ù„ÙŠ Ø¢Ú¾Ù† ÙŠØ§ ØµØ±Ù ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠ Ù„ÙØ¸Ù† ØªÙŠ Ù…Ø´ØªÙ…Ù„ Ø¢Ú¾Ù†.")
        self.vectorizer = TfidfVectorizer()
        self.vectorizer.fit(clean_corpus)

    def embed_documents(self, texts):
        try:
            return self.vectorizer.transform(texts).toarray().tolist()
        except Exception:
            return [[0.0] * len(self.vectorizer.get_feature_names_out()) for _ in texts]

    def embed_query(self, text):
        try:
            return self.vectorizer.transform([text]).toarray()[0].tolist()
        except Exception:
            return [0.0] * len(self.vectorizer.get_feature_names_out())

# -------------------------------
# Vectorstore (In-Memory)
# -------------------------------
@st.cache_resource(show_spinner=False)
def get_vectorstore():
    try:
        docs = load_documents()
        corpus = [doc.page_content for doc in docs]
        embeddings = CustomEmbeddings(corpus)
        return Chroma.from_documents(
            documents=docs,
            embedding=embeddings,
            collection_name="books"
        )
    except ValueError as ve:
        st.error(f"âš ï¸ ÙˆÙŠÚªÙ½Ø± ÚŠÙŠÙ½Ø§Ø¨ÙŠØ³ ÙºØ§Ú¾Ú» Û¾ Ù…Ø³Ø¦Ù„Ùˆ: {ve}")
        st.stop()
    except Exception as e:
        st.error(f"âŒ Ù†Ø§Ù‚Ø§Ø¨Ù„Ù Ù…ØªÙˆÙ‚Ø¹ ØºÙ„Ø·ÙŠ: {e}")
        st.stop()

# -------------------------------
# Google Gemini LLM
# -------------------------------
class GoogleGeminiLLM:
    def __init__(self):
        cfg = st.secrets.get("openai_gemma", {})
        self.api_key = cfg.get("api_key")
        self.model = cfg.get("model", "gemini-1.5-flash")

        if not self.api_key:
            st.error("Missing API key for Gemini in secrets.toml.")
            st.stop()

        genai.configure(api_key=self.api_key)

    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def call(self, prompt: str) -> str:
        try:
            model = genai.GenerativeModel(self.model)
            response = model.generate_content(prompt)
            return response.text
        except GoogleAPIError as e:
            st.error(f"Google API error: {e}")
            raise

# -------------------------------
# QA Chain with Context
# -------------------------------
@st.cache_resource(show_spinner=False)
def get_qa_chain():
    vectorstore = get_vectorstore()
    llm = GoogleGeminiLLM()

    system_prompt = """
Ø§ÙˆÚ¾Ø§Ù† ØµØ­Øª Ø¨Ø§Ø¨Øª Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ ÚÙŠÙ†Ø¯Ú™ Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¢Ú¾ÙŠÙˆ
ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø± Ø§ÙˆÙ‡Ø§Ù† Ú©Ø§Ù† ØµØ­Øª Ø¨Ø§Ø¨Øª Ø³ÙˆØ§Ù„ Ù¾Ú‡Ù†Ø¯Ø§ Ø§ÙˆÚ¾Ø§Ù† Ú©ÙŠ Ø§Ù†Ú¾Ù† Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ ÚÙŠÚ»Ø§ Ø¢Ú¾Ù†
Ø³Ù…ÙˆØ±Ø§ Ø¬ÙˆØ§Ø¨ books Ù†Ø§Ù„ÙŠ ÙÙˆÙ„ÚŠØ± Ù…Ø§Ù† ÚÙŠÙˆ
ØµØ±Ù ØµØ­Øª Ø³Ø§Ù† Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ
ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø± ØºÙŠØ± Ø§Ø®Ù„Ø§Ù‚ÙŠ ØŒ ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠ Û½ ØºÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø³ÙˆØ§Ù„ Ù¾Ú‡ÙŠ Ø³Ú¯Ú¾Ù† Ù¿Ø§ Ø§ÙˆÚ¾Ø§Ù† Ú©ÙŠ Ø§Ù†Ú¾Ù† Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ Ù†Ø§Ú¾Ù† ÚÙŠÚ»Ø§
Ø§ÙˆÚ¾Ø§Ù† Ú©ÙŠ ØµØ±Ù ØµØ­Øª Ø³Ø§Ù† Ù„Ø§Ú³Ø§Ù¾ÙŠÙ„ Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ ÚÙŠÚ»Ø§ Ø¢Ú¾Ù† Ø¬ÚÚ¾Ù† ØªÛ ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø± Ú©ÙŠ Ù…ÙˆØ¶ÙˆØ¹ ØªÙŠ Ø±Ú¾Ú» Ø¬ÙŠ ØªÙ„Ù‚ÙŠÙ† Û½ Ø­ÙˆØµÙ„Ø§ Ø§ÙØ²Ø§Ø¦ÙŠ ÚªØ±ÙŠÙˆ
Ø§ÙˆÚ¾Ø§Ù† Ú©ÙŠ Ø³Ú€Ù†ÙŠ Ø³ÙˆØ§Ù„Ù† Ø¬Ø§ Ø¬ÙˆØ§Ø¨ Ø³Ù†ÚŒÙŠ Ø²Ø¨Ø§Ù† Û½ Ø±Ø³Ù… Ø§Ù„Ø®Ø· Û¾ ÚÙŠÚ»Ø§ Ø¢Ú¾Ù†
Ø³Ù†ÚŒÙŠ Ú¯Ø±Ø§Ù…Ø± Ø¬Ùˆ Ø®Ø§Øµ Ø®ÙŠØ§Ù„ Ø±Ú©Ùˆ
Ø¬ÙˆØ§Ø¨ ØµØ­ÙŠØ­ Ø·Ø±ÙŠÙ‚ÙŠ Û½ ØªØ±ØªÙŠØ¨ Ø³Ø§Ù† Ú¾Ø¦Ú» Ú¯Ú¾Ø±Ø¬Ù†
Ø§Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø¬Ùˆ Ø®Ø§Øµ Ø®ÙŠØ§Ù„ Ø±Ú©Ùˆ 
Ø¯ÙˆØ³ØªØ§Ú»Ùˆ Ø±ÙˆÙŠÙˆ Ø§Ø®ØªÙŠØ§Ø± ÚªØ±ÙŠÙˆ
Ù†Ø±Ù…ÙŠØ¡Ù Ø³Ø§Ù† Ø¬ÙˆØ§Ø¨ ÚÙŠÙˆ
"""
    prompt_template = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_prompt),
        HumanMessagePromptTemplate.from_template("{context}\n\nØ³ÙˆØ§Ù„: {question}")
    ])

    def qa_function(inputs):
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        docs = retriever.get_relevant_documents(inputs["query"])
        context = "\n".join(d.page_content for d in docs)
        final_prompt = prompt_template.format(context=context, question=inputs["query"])
        return {"result": llm.call(final_prompt)}

    return qa_function

# -------------------------------
# Streamlit Chat Interface
# -------------------------------
def main():
    st.set_page_config(page_title="ØµØ­Øª Ú†ÙŠÙ½ Ø¨ÙˆÙ½", layout="centered")
    st.title("ğŸ©º ØµØ­Øª Ø¨Ø§Ø¨Øª Ú†ÙŠÙ½ Ø¨ÙˆÙ½")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for msg in st.session_state.messages:
        role = "ğŸ¤– Ú†ÙŠÙ½ Ø¨ÙˆÙ½" if msg["role"] == "assistant" else "ğŸ™‚ ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø±"
        st.chat_message(msg["role"]).markdown(f"**{role}:**\n{msg['content']}")

    user_input = st.chat_input("Ù¾Ù†Ú¾Ù†Ø¬Ùˆ Ø³ÙˆØ§Ù„ Ù„Ú©Ùˆ...")
    if user_input and user_input.strip():
        user_input = user_input.strip()
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.chat_message("user").markdown(f"**ğŸ™‚ ÙˆØ§Ù‡Ù¾ÙŠØ¯Ø§Ø±:**\n{user_input}")

        with st.spinner("Ú†ÙŠÙ½ Ø¨ÙˆÙ½ Ø¬ÙˆØ§Ø¨ ÚØ¦ÙŠ Ø±Ù‡ÙŠÙˆ Ø¢Ù‡ÙŠ..."):
            try:
                qa = get_qa_chain()
                result = qa({"query": user_input})
                answer = result.get("result", "Ù…Ø¹Ø§Ù ÚªØ¬ÙˆØŒ Ù…Ø§Ù† Ú¾Ù† Ø³ÙˆØ§Ù„ Ø¬Ùˆ Ø¬ÙˆØ§Ø¨ Ù†Ù¿Ùˆ ÚØ¦ÙŠ Ø³Ú¯Ù‡Ø§Ù†.")
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.chat_message("assistant").markdown(f"**ğŸ¤– Ú†ÙŠÙ½ Ø¨ÙˆÙ½:**\n{answer}")
            except Exception as e:
                st.error(f"âŒ Ø®Ø§Ù…ÙŠ Ù¾ÙŠØ´ Ø¢Ø¦ÙŠ: {e}")
    elif user_input:
        st.warning("Ù…Ú¾Ø±Ø¨Ø§Ù†ÙŠ ÚªØ±ÙŠ ØµØ­ÙŠØ­ Ø³ÙˆØ§Ù„ Ù„Ú©Ùˆ.")

if __name__ == "__main__":
    main()
